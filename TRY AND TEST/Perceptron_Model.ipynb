{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c679538a",
   "metadata": {},
   "source": [
    "### Introduction to artificial intelligence\n",
    "\n",
    "This notebook contains a Script with a perceptron implementation\n",
    "\n",
    "Author:\n",
    "    Oscar Hernández Terán\n",
    "    \n",
    "Bachelor's Degree:\n",
    "    Geophysical Engineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6cc2a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the libraries\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f8978d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The name of the values are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We load the dataset \n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "print('The name of the values are:')\n",
    "list(iris.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "059cea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'sklearn.utils.Bunch'>\n"
     ]
    }
   ],
   "source": [
    "print(type(iris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7216eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We cut and store values in a new variable\n",
    "X = iris.data[:, (2,3)] # Petal length, petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bd0542c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The values 0, 1 and 2 corresponds to 'iris.target_names' respectively.\n",
    "iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f61be55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We create a Boolean array from the values of 'iris.target'\n",
    "y = (iris.target == 0)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7167907a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From this boolean matrix we create a binary matrix\n",
    "y = y.astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46ebf5c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.4, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.3, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.7, 0.4],\n",
       "       [1.4, 0.3],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.5, 0.1],\n",
       "       [1.5, 0.2],\n",
       "       [1.6, 0.2],\n",
       "       [1.4, 0.1],\n",
       "       [1.1, 0.1],\n",
       "       [1.2, 0.2],\n",
       "       [1.5, 0.4],\n",
       "       [1.3, 0.4],\n",
       "       [1.4, 0.3],\n",
       "       [1.7, 0.3],\n",
       "       [1.5, 0.3],\n",
       "       [1.7, 0.2],\n",
       "       [1.5, 0.4],\n",
       "       [1. , 0.2],\n",
       "       [1.7, 0.5],\n",
       "       [1.9, 0.2],\n",
       "       [1.6, 0.2],\n",
       "       [1.6, 0.4],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.6, 0.2],\n",
       "       [1.6, 0.2],\n",
       "       [1.5, 0.4],\n",
       "       [1.5, 0.1],\n",
       "       [1.4, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.2, 0.2],\n",
       "       [1.3, 0.2],\n",
       "       [1.4, 0.1],\n",
       "       [1.3, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.3, 0.3],\n",
       "       [1.3, 0.3],\n",
       "       [1.3, 0.2],\n",
       "       [1.6, 0.6],\n",
       "       [1.9, 0.4],\n",
       "       [1.4, 0.3],\n",
       "       [1.6, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [1.5, 0.2],\n",
       "       [1.4, 0.2],\n",
       "       [4.7, 1.4],\n",
       "       [4.5, 1.5],\n",
       "       [4.9, 1.5],\n",
       "       [4. , 1.3],\n",
       "       [4.6, 1.5],\n",
       "       [4.5, 1.3],\n",
       "       [4.7, 1.6],\n",
       "       [3.3, 1. ],\n",
       "       [4.6, 1.3],\n",
       "       [3.9, 1.4],\n",
       "       [3.5, 1. ],\n",
       "       [4.2, 1.5],\n",
       "       [4. , 1. ],\n",
       "       [4.7, 1.4],\n",
       "       [3.6, 1.3],\n",
       "       [4.4, 1.4],\n",
       "       [4.5, 1.5],\n",
       "       [4.1, 1. ],\n",
       "       [4.5, 1.5],\n",
       "       [3.9, 1.1],\n",
       "       [4.8, 1.8],\n",
       "       [4. , 1.3],\n",
       "       [4.9, 1.5],\n",
       "       [4.7, 1.2],\n",
       "       [4.3, 1.3],\n",
       "       [4.4, 1.4],\n",
       "       [4.8, 1.4],\n",
       "       [5. , 1.7],\n",
       "       [4.5, 1.5],\n",
       "       [3.5, 1. ],\n",
       "       [3.8, 1.1],\n",
       "       [3.7, 1. ],\n",
       "       [3.9, 1.2],\n",
       "       [5.1, 1.6],\n",
       "       [4.5, 1.5],\n",
       "       [4.5, 1.6],\n",
       "       [4.7, 1.5],\n",
       "       [4.4, 1.3],\n",
       "       [4.1, 1.3],\n",
       "       [4. , 1.3],\n",
       "       [4.4, 1.2],\n",
       "       [4.6, 1.4],\n",
       "       [4. , 1.2],\n",
       "       [3.3, 1. ],\n",
       "       [4.2, 1.3],\n",
       "       [4.2, 1.2],\n",
       "       [4.2, 1.3],\n",
       "       [4.3, 1.3],\n",
       "       [3. , 1.1],\n",
       "       [4.1, 1.3],\n",
       "       [6. , 2.5],\n",
       "       [5.1, 1.9],\n",
       "       [5.9, 2.1],\n",
       "       [5.6, 1.8],\n",
       "       [5.8, 2.2],\n",
       "       [6.6, 2.1],\n",
       "       [4.5, 1.7],\n",
       "       [6.3, 1.8],\n",
       "       [5.8, 1.8],\n",
       "       [6.1, 2.5],\n",
       "       [5.1, 2. ],\n",
       "       [5.3, 1.9],\n",
       "       [5.5, 2.1],\n",
       "       [5. , 2. ],\n",
       "       [5.1, 2.4],\n",
       "       [5.3, 2.3],\n",
       "       [5.5, 1.8],\n",
       "       [6.7, 2.2],\n",
       "       [6.9, 2.3],\n",
       "       [5. , 1.5],\n",
       "       [5.7, 2.3],\n",
       "       [4.9, 2. ],\n",
       "       [6.7, 2. ],\n",
       "       [4.9, 1.8],\n",
       "       [5.7, 2.1],\n",
       "       [6. , 1.8],\n",
       "       [4.8, 1.8],\n",
       "       [4.9, 1.8],\n",
       "       [5.6, 2.1],\n",
       "       [5.8, 1.6],\n",
       "       [6.1, 1.9],\n",
       "       [6.4, 2. ],\n",
       "       [5.6, 2.2],\n",
       "       [5.1, 1.5],\n",
       "       [5.6, 1.4],\n",
       "       [6.1, 2.3],\n",
       "       [5.6, 2.4],\n",
       "       [5.5, 1.8],\n",
       "       [4.8, 1.8],\n",
       "       [5.4, 2.1],\n",
       "       [5.6, 2.4],\n",
       "       [5.1, 2.3],\n",
       "       [5.1, 1.9],\n",
       "       [5.9, 2.3],\n",
       "       [5.7, 2.5],\n",
       "       [5.2, 2.3],\n",
       "       [5. , 1.9],\n",
       "       [5.2, 2. ],\n",
       "       [5.4, 2.3],\n",
       "       [5.1, 1.8]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Petal length, petal width\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7a1424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the Perceptron module from sklearn\n",
    "from sklearn.linear_model import Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e306b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Perceptron in module sklearn.linear_model._perceptron:\n",
      "\n",
      "class Perceptron(sklearn.linear_model._stochastic_gradient.BaseSGDClassifier)\n",
      " |  Perceptron(*, penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)\n",
      " |  \n",
      " |  Perceptron\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <perceptron>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |  penalty : {'l2','l1','elasticnet'}, default=None\n",
      " |      The penalty (aka regularization term) to be used.\n",
      " |  \n",
      " |  alpha : float, default=0.0001\n",
      " |      Constant that multiplies the regularization term if regularization is\n",
      " |      used.\n",
      " |  \n",
      " |  l1_ratio : float, default=0.15\n",
      " |      The Elastic Net mixing parameter, with `0 <= l1_ratio <= 1`.\n",
      " |      `l1_ratio=0` corresponds to L2 penalty, `l1_ratio=1` to L1.\n",
      " |      Only used if `penalty='elasticnet'`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether the intercept should be estimated or not. If False, the\n",
      " |      data is assumed to be already centered.\n",
      " |  \n",
      " |  max_iter : int, default=1000\n",
      " |      The maximum number of passes over the training data (aka epochs).\n",
      " |      It only impacts the behavior in the ``fit`` method, and not the\n",
      " |      :meth:`partial_fit` method.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  tol : float, default=1e-3\n",
      " |      The stopping criterion. If it is not None, the iterations will stop\n",
      " |      when (loss > previous_loss - tol).\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  shuffle : bool, default=True\n",
      " |      Whether or not the training data should be shuffled after each epoch.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      The verbosity level\n",
      " |  \n",
      " |  eta0 : double, default=1\n",
      " |      Constant by which the updates are multiplied.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of CPUs to use to do the OVA (One Versus All, for\n",
      " |      multi-class problems) computation.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Used to shuffle the training data, when ``shuffle`` is set to\n",
      " |      ``True``. Pass an int for reproducible output across multiple\n",
      " |      function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  early_stopping : bool, default=False\n",
      " |      Whether to use early stopping to terminate training when validation.\n",
      " |      score is not improving. If set to True, it will automatically set aside\n",
      " |      a stratified fraction of training data as validation and terminate\n",
      " |      training when validation score is not improving by at least tol for\n",
      " |      n_iter_no_change consecutive epochs.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  n_iter_no_change : int, default=5\n",
      " |      Number of iterations with no improvement to wait before early stopping.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  class_weight : dict, {class_label: weight} or \"balanced\", default=None\n",
      " |      Preset for the class_weight fit parameter.\n",
      " |  \n",
      " |      Weights associated with classes. If not given, all classes\n",
      " |      are supposed to have weight one.\n",
      " |  \n",
      " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
      " |      weights inversely proportional to class frequencies in the input data\n",
      " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to True, reuse the solution of the previous call to fit as\n",
      " |      initialization, otherwise, just erase the previous solution. See\n",
      " |      :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The unique classes labels.\n",
      " |  \n",
      " |  coef_ : ndarray of shape (1, n_features) if n_classes == 2 else             (n_classes, n_features)\n",
      " |      Weights assigned to the features.\n",
      " |  \n",
      " |  intercept_ : ndarray of shape (1,) if n_classes == 2 else (n_classes,)\n",
      " |      Constants in decision function.\n",
      " |  \n",
      " |  loss_function_ : concrete LossFunction\n",
      " |      The function that determines the loss, or difference between the\n",
      " |      output of the algorithm and the target values.\n",
      " |  \n",
      " |  n_iter_ : int\n",
      " |      The actual number of iterations to reach the stopping criterion.\n",
      " |      For multiclass fits, it is the maximum over every binary fit.\n",
      " |  \n",
      " |  t_ : int\n",
      " |      Number of weight updates performed during training.\n",
      " |      Same as ``(n_iter_ * n_samples)``.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  \n",
      " |  ``Perceptron`` is a classification algorithm which shares the same\n",
      " |  underlying implementation with ``SGDClassifier``. In fact,\n",
      " |  ``Perceptron()`` is equivalent to `SGDClassifier(loss=\"perceptron\",\n",
      " |  eta0=1, learning_rate=\"constant\", penalty=None)`.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.datasets import load_digits\n",
      " |  >>> from sklearn.linear_model import Perceptron\n",
      " |  >>> X, y = load_digits(return_X_y=True)\n",
      " |  >>> clf = Perceptron(tol=1e-3, random_state=0)\n",
      " |  >>> clf.fit(X, y)\n",
      " |  Perceptron()\n",
      " |  >>> clf.score(X, y)\n",
      " |  0.939...\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SGDClassifier\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  \n",
      " |  https://en.wikipedia.org/wiki/Perceptron and references therein.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Perceptron\n",
      " |      sklearn.linear_model._stochastic_gradient.BaseSGDClassifier\n",
      " |      sklearn.linear_model._base.LinearClassifierMixin\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      sklearn.linear_model._stochastic_gradient.BaseSGD\n",
      " |      sklearn.linear_model._base.SparseCoefMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, penalty=None, alpha=0.0001, l1_ratio=0.15, fit_intercept=True, max_iter=1000, tol=0.001, shuffle=True, verbose=0, eta0=1.0, n_jobs=None, random_state=0, early_stopping=False, validation_fraction=0.1, n_iter_no_change=5, class_weight=None, warm_start=False)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      " |  \n",
      " |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
      " |      Fit linear model with Stochastic Gradient Descent.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Training data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Target values.\n",
      " |      \n",
      " |      coef_init : ndarray of shape (n_classes, n_features), default=None\n",
      " |          The initial coefficients to warm-start the optimization.\n",
      " |      \n",
      " |      intercept_init : ndarray of shape (n_classes,), default=None\n",
      " |          The initial intercept to warm-start the optimization.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples.\n",
      " |          If not provided, uniform weights are assumed. These weights will\n",
      " |          be multiplied with class_weight (passed through the\n",
      " |          constructor) if class_weight is specified.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self :\n",
      " |          Returns an instance of self.\n",
      " |  \n",
      " |  partial_fit(self, X, y, classes=None, sample_weight=None)\n",
      " |      Perform one epoch of stochastic gradient descent on given samples.\n",
      " |      \n",
      " |      Internally, this method uses ``max_iter = 1``. Therefore, it is not\n",
      " |      guaranteed that a minimum of the cost function is reached after calling\n",
      " |      it once. Matters such as objective convergence and early stopping\n",
      " |      should be handled by the user.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          Subset of the training data.\n",
      " |      \n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          Subset of the target values.\n",
      " |      \n",
      " |      classes : ndarray of shape (n_classes,), default=None\n",
      " |          Classes across all calls to partial_fit.\n",
      " |          Can be obtained by via `np.unique(y_all)`, where y_all is the\n",
      " |          target vector of the entire dataset.\n",
      " |          This argument is required for the first call to partial_fit\n",
      " |          and can be omitted in the subsequent calls.\n",
      " |          Note that y doesn't need to contain all labels in `classes`.\n",
      " |      \n",
      " |      sample_weight : array-like, shape (n_samples,), default=None\n",
      " |          Weights applied to individual samples.\n",
      " |          If not provided, uniform weights are assumed.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self :\n",
      " |          Returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.linear_model._stochastic_gradient.BaseSGDClassifier:\n",
      " |  \n",
      " |  loss_functions = {'epsilon_insensitive': (<class 'sklearn.linear_model...\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.LinearClassifierMixin:\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Predict confidence scores for samples.\n",
      " |      \n",
      " |      The confidence score for a sample is proportional to the signed\n",
      " |      distance of that sample to the hyperplane.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)\n",
      " |          Confidence scores per (sample, class) combination. In the binary\n",
      " |          case, confidence score for self.classes_[1] where >0 means this\n",
      " |          class would be predicted.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class labels for samples in X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape [n_samples]\n",
      " |          Predicted class label per sample.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      " |  \n",
      " |  set_params(self, **kwargs)\n",
      " |      Set and validate the parameters of estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **kwargs : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from sklearn.linear_model._stochastic_gradient.BaseSGD:\n",
      " |  \n",
      " |  average_coef_\n",
      " |  \n",
      " |  average_intercept_\n",
      " |  \n",
      " |  standard_coef_\n",
      " |  \n",
      " |  standard_intercept_\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.linear_model._base.SparseCoefMixin:\n",
      " |  \n",
      " |  densify(self)\n",
      " |      Convert coefficient matrix to dense array format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member (back) to a numpy.ndarray. This is the\n",
      " |      default format of ``coef_`` and is required for fitting, so calling\n",
      " |      this method is only required on models that have previously been\n",
      " |      sparsified; otherwise, it is a no-op.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  sparsify(self)\n",
      " |      Convert coefficient matrix to sparse format.\n",
      " |      \n",
      " |      Converts the ``coef_`` member to a scipy.sparse matrix, which for\n",
      " |      L1-regularized models can be much more memory- and storage-efficient\n",
      " |      than the usual numpy.ndarray representation.\n",
      " |      \n",
      " |      The ``intercept_`` member is not converted.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |          Fitted estimator.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      For non-sparse models, i.e. when there are not many zeros in ``coef_``,\n",
      " |      this may actually *increase* memory usage, so use this method with\n",
      " |      care. A rule of thumb is that the number of zero elements, which can\n",
      " |      be computed with ``(coef_ == 0).sum()``, must be more than 50% for this\n",
      " |      to provide significant benefits.\n",
      " |      \n",
      " |      After calling this method, further fitting with the partial_fit\n",
      " |      method (if any) will not work until you call densify.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can check the information of any module in Python with the command 'help()'\n",
    "help(Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9ec098c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Perceptron(random_state=42)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use a seed to reproduce data\n",
    "per_clf = Perceptron(random_state=42)\n",
    "# \n",
    "# |  fit(self, X, y, coef_init=None, intercept_init=None, sample_weight=None)\n",
    "# |      Fit linear model with Stochastic Gradient Descent.\n",
    "per_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67ca9b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict class labels for samples in X.\n",
    "y_pred = per_clf.predict(X)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "152406d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function accuracy_score in module sklearn.metrics._classification:\n",
      "\n",
      "accuracy_score(y_true, y_pred, *, normalize=True, sample_weight=None)\n",
      "    Accuracy classification score.\n",
      "    \n",
      "    In multilabel classification, this function computes subset accuracy:\n",
      "    the set of labels predicted for a sample must *exactly* match the\n",
      "    corresponding set of labels in y_true.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <accuracy_score>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    y_true : 1d array-like, or label indicator array / sparse matrix\n",
      "        Ground truth (correct) labels.\n",
      "    \n",
      "    y_pred : 1d array-like, or label indicator array / sparse matrix\n",
      "        Predicted labels, as returned by a classifier.\n",
      "    \n",
      "    normalize : bool, default=True\n",
      "        If ``False``, return the number of correctly classified samples.\n",
      "        Otherwise, return the fraction of correctly classified samples.\n",
      "    \n",
      "    sample_weight : array-like of shape (n_samples,), default=None\n",
      "        Sample weights.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    score : float\n",
      "        If ``normalize == True``, return the fraction of correctly\n",
      "        classified samples (float), else returns the number of correctly\n",
      "        classified samples (int).\n",
      "    \n",
      "        The best performance is 1 with ``normalize == True`` and the number\n",
      "        of samples with ``normalize == False``.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    jaccard_score, hamming_loss, zero_one_loss\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    In binary and multiclass classification, this function is equal\n",
      "    to the ``jaccard_score`` function.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn.metrics import accuracy_score\n",
      "    >>> y_pred = [0, 2, 1, 3]\n",
      "    >>> y_true = [0, 1, 2, 3]\n",
      "    >>> accuracy_score(y_true, y_pred)\n",
      "    0.5\n",
      "    >>> accuracy_score(y_true, y_pred, normalize=False)\n",
      "    2\n",
      "    \n",
      "    In the multilabel case with binary label indicators:\n",
      "    \n",
      "    >>> import numpy as np\n",
      "    >>> accuracy_score(np.array([[0, 1], [1, 1]]), np.ones((2, 2)))\n",
      "    0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We import the 'accuracy_score' module from sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "help(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c80418f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    Accuracy classification score.\n",
    "    \n",
    "    In multilabel classification, this function computes subset accuracy:\n",
    "    the set of labels predicted for a sample must *exactly* match the\n",
    "    corresponding set of labels in y_true.\n",
    "'''\n",
    "accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ce61fdc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4, -2.2]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Weights assigned to the features.\n",
    "per_clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a8c6e3c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Constants in decision function.\n",
    "per_clf.intercept_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
